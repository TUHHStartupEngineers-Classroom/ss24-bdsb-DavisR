[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Patent Dominance: What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.\nAnswer:\nFor this we use the reduced data set due to computational restrictions. The data sets will be read in and wrangled and filtered according to US-company specific keywords. Then the companies with the most patents will be printed.\nCode:\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(vroom)\nlibrary(magrittr) \n\n\n# Challenge\ncol_types &lt;- list(\n  id = col_character(),\n  type = col_character(),\n  number = col_character(),\n  country = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  abstract = col_character(),\n  title = col_character(),\n  kind = col_character(),\n  num_claims = col_double(),\n  filename = col_character(),\n  withdrawn = col_double()\n)\n\npatent_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: type,\n#&gt; number, country, abstract, title, kind, filename, withdrawn\n\npatent_assignee_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: id, type,\n#&gt; number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\nassignee_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: number,\n#&gt; country, date, abstract, title, kind, num_claims, filename, withdrawn\n\nuspc_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: id, type,\n#&gt; number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\n# Question 1\n# For 1 assignee, patent_assignee\n# Merge tables and count patents per organization\nmerged_tbl &lt;- merge(assignee_tbl, patent_assignee_tbl, by.x = \"id\", by.y = \"assignee_id\", all.x = TRUE)\n\n# Filter US companies and count patents\n# US companies are being filtered by US specific keywords\npatent_counts &lt;- merged_tbl %&gt;%\n  filter(grepl(\"Inc\\\\.|LLC|Corporation|Corporated|Company|Ltd\\\\.|Incorporated\", organization, ignore.case = TRUE)) %&gt;%\n  group_by(organization) %&gt;%\n  summarise(patent_count = n()) %&gt;%\n  arrange(desc(patent_count))\n\n# Print the result\nprint(patent_counts)\n\n#&gt; # A tibble: 28,689 × 2\n#&gt;    organization                                patent_count\n#&gt;    &lt;chr&gt;                                              &lt;int&gt;\n#&gt;  1 International Business Machines Corporation         7547\n#&gt;  2 Samsung Electronics Co., Ltd.                       5835\n#&gt;  3 Sony Corporation                                    3326\n#&gt;  4 Microsoft Corporation                               3165\n#&gt;  5 Google Inc.                                         2668\n#&gt;  6 QUALCOMM Incorporated                               2597\n#&gt;  7 LG Electronics Inc.                                 2459\n#&gt;  8 Panasonic Corporation                               2218\n#&gt;  9 Apple Inc.                                          2201\n#&gt; 10 General Electric Company                            1873\n#&gt; # ℹ 28,679 more rows\n\n\n\nQuestion:\nRecent patent activity: What US company had the most patents granted in August 2014? List the top 10 companies with the most new granted patents for August 2014.\nAnswer:\nFor this I filtered the patents for August 2014 and US only. Then I summarized the number of their patents to get the companies that had the most patents. These are printed out.\nCode:\n\n# For 2 assignee, patent_assignee, patent\n# Filter patents granted in August 2014\naugust_patents &lt;- patent_tbl %&gt;%\n  filter(format(date, \"%Y-%m\") == \"2014-08\")\n\n# Join with patent_assignee_tbl to get assignee information\npatents_assignees &lt;- august_patents %&gt;%\n  inner_join(patent_assignee_tbl, by = c(\"id\" = \"patent_id\"))\n\n# Join with assignee_tbl to get assignee information\npatents_assignees_details &lt;- patents_assignees %&gt;%\n  inner_join(assignee_tbl, by = c(\"assignee_id\" = \"id\"))\n\n# Filter only US companies\nus_companies &lt;- patents_assignees_details %&gt;%\n  filter(grepl(\"USA\", organization, ignore.case = TRUE))\n\n# Count patents granted to each US company\npatents_count &lt;- us_companies %&gt;%\n  group_by(organization) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents))\n\n# Select top 10 companies\ntop_10_us_companies &lt;- head(patents_count, 10)\n\nprint(top_10_us_companies)\n\n#&gt; # A tibble: 10 × 2\n#&gt;    organization                                                      num_patents\n#&gt;    &lt;chr&gt;                                                                   &lt;int&gt;\n#&gt;  1 Siemens Medical Solutions USA, Inc.                                        12\n#&gt;  2 ECOLAB USA INC.                                                             9\n#&gt;  3 HITACHI KOKUSAI ELECTRIC INC.                                               8\n#&gt;  4 United Services Automobile Association (USAA)                               6\n#&gt;  5 Ecole Polytechnique Federale de Lausanne (EPFL)                             3\n#&gt;  6 Philip Morris USA Inc.                                                      3\n#&gt;  7 Yissum Research Development Company of the Hebrew University of …           3\n#&gt;  8 CAUSAM ENERGY, INC.                                                         2\n#&gt;  9 Hendrickson USA, L.L.C.                                                     2\n#&gt; 10 Musashi Engineering, Inc.                                                   2\n\n\n\nQuestion:\nInnovation in Tech: What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?\nAnswer:\nFor this the the companies with the most number of patents will be taken and then for those the top company classes will be taken to be printed out, which are the top 5 tech classes of the top 10 companies.\nCode:\n\n# Question 3\n# For 3 assignee_tbl, patent_assignee_tbl, uspc_tbl\n# Find top 10 companies with most patents\ntop_10_companies &lt;- patent_assignee_tbl %&gt;%\n  group_by(assignee_id) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents)) %&gt;%\n  top_n(10)\n\n#&gt; Selecting by num_patents\n\n# Join with assignee_tbl to get org information\ntop_10_companies_with_org &lt;- top_10_companies %&gt;%\n  inner_join(select(assignee_tbl, -type), by = c(\"assignee_id\" = \"id\"))\n\n\n# Main classes of patents\ntop_10_main_classes &lt;- top_10_companies_with_org %&gt;%\n  inner_join(patent_assignee_tbl, by = \"assignee_id\") %&gt;%\n  inner_join(uspc_tbl %&gt;% mutate(patent_id = as.character(patent_id)), by = \"patent_id\") %&gt;%\n  group_by(mainclass_id) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents)) %&gt;%\n  top_n(5)\n\n#&gt; Warning in inner_join(., uspc_tbl %&gt;% mutate(patent_id = as.character(patent_id)), : Detected an unexpected many-to-many relationship between `x` and `y`.\n#&gt; ℹ Row 1 of `x` matches multiple rows in `y`.\n#&gt; ℹ Row 373502 of `y` matches multiple rows in `x`.\n#&gt; ℹ If a many-to-many relationship is expected, set `relationship =\n#&gt;   \"many-to-many\"` to silence this warning.\n\n\n#&gt; Selecting by num_patents\n\nprint(top_10_main_classes)\n\n#&gt; # A tibble: 5 × 2\n#&gt;   mainclass_id num_patents\n#&gt;          &lt;dbl&gt;       &lt;int&gt;\n#&gt; 1          257        7956\n#&gt; 2          455        6120\n#&gt; 3          370        5448\n#&gt; 4          348        4102\n#&gt; 5          709        4010"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "1 Challenge 1\nQuestion 1:\nAnalyze the sales by location (state) with a bar plot. Since state and city are multiple features (variables), they should be split. Which state has the highest revenue?\nAnswer:\nFor this I imported the given .xlsx-files, joined and wrangled them, to get a data set for the sales by the location.\nCode:\n\n# Challenge 1\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"readxl\")\n\n# Importing Files\nbikes_tbl      &lt;- read_excel(path = \"../../scripts/01_bike_sales/01_raw_data/bikes.xlsx\")\nbikeshops_tbl  &lt;- read_excel(path = \"../../scripts/01_bike_sales/01_raw_data/bikeshops.xlsx\")\norderlines_tbl &lt;- read_excel(path = \"../../scripts/01_bike_sales/01_raw_data/orderlines.xlsx\")\n\n#&gt; New names:\n#&gt; • `` -&gt; `...1`\n\n# Joining Data\nleft_join(orderlines_tbl, bikes_tbl, by = c(\"product.id\" = \"bike.id\"))\n\n\n\n  \n\n\n# Chaining commands with the pipe and assigning it to order_items_joined_tbl\nbike_orderlines_joined_tbl &lt;- orderlines_tbl %&gt;%\n  left_join(bikes_tbl, by = c(\"product.id\" = \"bike.id\")) %&gt;%\n  left_join(bikeshops_tbl, by = c(\"customer.id\" = \"bikeshop.id\"))\n\n# Wrangling Data\nbike_orderlines_joined_tbl %&gt;% \n  select(category) %&gt;%\n  filter(str_detect(category, \"^Mountain\")) %&gt;% \n  unique()\n\n\n\n  \n\n\nbike_orderlines_wrangled_tbl &lt;- bike_orderlines_joined_tbl %&gt;%\n  separate(col    = category,\n           into   = c(\"category.1\", \"category.2\", \"category.3\"),\n           sep    = \" - \") %&gt;%\n  mutate(total.price = price * quantity) %&gt;%\n  select(-...1, -gender) %&gt;%\n  select(-ends_with(\".id\")) %&gt;%\n  bind_cols(bike_orderlines_joined_tbl %&gt;% select(order.id)) %&gt;% \n  select(order.id, contains(\"order\"), contains(\"model\"), contains(\"category\"),\n         price, quantity, total.price,\n         everything()) %&gt;%\n  rename(bikeshop = name) %&gt;%\n  set_names(names(.) %&gt;% str_replace_all(\"\\\\.\", \"_\"))\n\n# Separate cities and states\nbike_orderlines_wrangled_tbl_states &lt;- separate(bike_orderlines_wrangled_tbl,\n                                                col = location,\n                                                into = c(\"city\", \"state\"),\n                                                sep = \", \",\n                                                convert = T)\nbike_orderlines_wrangled_tbl_states\n\n\n\n  \n\n\nsales_by_year_and_state_tbl &lt;- bike_orderlines_wrangled_tbl_states %&gt;%\n  select(order_date, total_price, state) %&gt;%\n  mutate(year = year(order_date)) %&gt;%\n  group_by(state) %&gt;% \n  summarize(sales = sum(total_price)) %&gt;%\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\nsales_by_year_and_state_tbl\n\n\n\n  \n\n\nsales_by_year_and_state_tbl %&gt;%\n  ggplot(aes(x = state, y = sales)) +\n  \n  # Geometries\n  geom_col(fill = \"#2DC6D6\") + # Use geom_col for a bar plot\n  geom_label(aes(label = sales_text)) + # Adding labels to the bars\n  geom_smooth(method = \"lm\", se = FALSE) + # Adding a trendline\n  \n  # Formatting\n  # scale_y_continuous(labels = scales::dollar) + # Change the y-axis. \n  # Again, we have to adjust it for euro values\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title    = \"Revenue by state\",\n    x = \"States\",\n    y = \"Revenue\"\n  ) + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n2 Challenge 2\nQuestion 2:\nAnalyze the sales by location and year (facet_wrap). Because there are 12 states with bike stores, you should get 12 plots.\nAnswer:\nForm this I made a data set for the sales according to the states per year and plotted it using facet_wrap to get 12 plots for every location and year.\nCode:\n\n# Get state's sales per year\nsales_by_state_in_year  &lt;- separate(bike_orderlines_wrangled_tbl,\n                                    col = location,\n                                    into = c(\"city\", \"state\"),\n                                    sep = \", \",\n                                    convert = T)\n\nsales_by_state_in_year\n\n\n\n  \n\n\nsales_by_state_in_year &lt;- sales_by_state_in_year %&gt;%\n  \n  # Select columns\n  select(order_date, total_price, state) %&gt;%\n  \n  # Add year column\n  mutate(year = year(order_date)) %&gt;%\n  \n  # Grouping by year and summarizing sales\n  group_by(state, year) %&gt;% \n  summarize(sales = sum(total_price)) %&gt;%\n  \n  # Add a column that turns the numbers into a currency format \n  # (makes it in the plot optically more appealing)\n  # mutate(sales_text = scales::dollar(sales)) &lt;- Works for dollar values\n  mutate(sales_text = scales::dollar(sales, big.mark = \".\", \n                                     decimal.mark = \",\", \n                                     prefix = \"\", \n                                     suffix = \" €\"))\n\n#&gt; `summarise()` has grouped output by 'state'. You can override using the\n#&gt; `.groups` argument.\n\nsales_by_state_in_year\n\n\n\n  \n\n\n# Plot\nsalesYearStatePlot &lt;- sales_by_state_in_year  %&gt;%\n  # Set up x, y, fill\n  ggplot(aes(x = year, y = sales, fill = state)) +\n  \n  # Geometries\n  geom_col() + # Run up to here to get a stacked bar plot\n  \n  # Facet\n  facet_wrap(~ state) +\n  \n  # Formatting\n  scale_y_continuous(labels = scales::dollar_format(big.mark = \".\", \n                                                    decimal.mark = \",\", \n                                                    prefix = \"\", \n                                                    suffix = \" €\")) +\n  labs(\n    title = \"Revenue by year and state\",\n    subtitle = \"Each state is ordered by years 2015 to 2019\",\n    fill = \"States:\"\n  ) + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nsalesYearStatePlot"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "2.1 Header 2",
    "text": "2.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "1 Challenge 1\nQuestion 1:\nGet some data via an API. There are millions of providers, that offer API access for free and have good documentation about how to query their service. You just have to google them. You can use whatever service you want. For example, you can get data about your listening history (spotify), get data about flights (skyscanner) or just check the weather forecast. Print the data in a readable format, e.g. a table if you want, you could also plot it.\nAnswer:\nFor this I used the weather api of open-meteo.com. The data that I get from the site is the weather forecast for 7 days at the location of the TUHH A-building. This can be found in the data set that is shown below and plotted too.\nCode:\n\nlibrary(RSQLite)\nlibrary(httr)\nlibrary(glue)\nlibrary(jsonlite)\nlibrary(\"tidyverse\")\nlibrary(rvest)\n\n# Challenge 1\n\n# Latitude and longitude of TUHH\nlatitude &lt;- \"53.46\"\nlongitude &lt;- \"9.97\"\n  \nresp &lt;- GET(glue(\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&hourly=temperature_2m&timezone=Europe%2FBerlin\"))\n\nrespTable &lt;- resp %&gt;% \n            .$content %&gt;% \n            rawToChar() %&gt;% \n            fromJSON()\nrespTable\n\n#&gt; $latitude\n#&gt; [1] 53.46\n#&gt; \n#&gt; $longitude\n#&gt; [1] 9.98\n#&gt; \n#&gt; $generationtime_ms\n#&gt; [1] 0.04005432\n#&gt; \n#&gt; $utc_offset_seconds\n#&gt; [1] 7200\n#&gt; \n#&gt; $timezone\n#&gt; [1] \"Europe/Berlin\"\n#&gt; \n#&gt; $timezone_abbreviation\n#&gt; [1] \"CEST\"\n#&gt; \n#&gt; $elevation\n#&gt; [1] 23\n#&gt; \n#&gt; $hourly_units\n#&gt; $hourly_units$time\n#&gt; [1] \"iso8601\"\n#&gt; \n#&gt; $hourly_units$temperature_2m\n#&gt; [1] \"°C\"\n#&gt; \n#&gt; \n#&gt; $hourly\n#&gt; $hourly$time\n#&gt;   [1] \"2024-05-18T00:00\" \"2024-05-18T01:00\" \"2024-05-18T02:00\"\n#&gt;   [4] \"2024-05-18T03:00\" \"2024-05-18T04:00\" \"2024-05-18T05:00\"\n#&gt;   [7] \"2024-05-18T06:00\" \"2024-05-18T07:00\" \"2024-05-18T08:00\"\n#&gt;  [10] \"2024-05-18T09:00\" \"2024-05-18T10:00\" \"2024-05-18T11:00\"\n#&gt;  [13] \"2024-05-18T12:00\" \"2024-05-18T13:00\" \"2024-05-18T14:00\"\n#&gt;  [16] \"2024-05-18T15:00\" \"2024-05-18T16:00\" \"2024-05-18T17:00\"\n#&gt;  [19] \"2024-05-18T18:00\" \"2024-05-18T19:00\" \"2024-05-18T20:00\"\n#&gt;  [22] \"2024-05-18T21:00\" \"2024-05-18T22:00\" \"2024-05-18T23:00\"\n#&gt;  [25] \"2024-05-19T00:00\" \"2024-05-19T01:00\" \"2024-05-19T02:00\"\n#&gt;  [28] \"2024-05-19T03:00\" \"2024-05-19T04:00\" \"2024-05-19T05:00\"\n#&gt;  [31] \"2024-05-19T06:00\" \"2024-05-19T07:00\" \"2024-05-19T08:00\"\n#&gt;  [34] \"2024-05-19T09:00\" \"2024-05-19T10:00\" \"2024-05-19T11:00\"\n#&gt;  [37] \"2024-05-19T12:00\" \"2024-05-19T13:00\" \"2024-05-19T14:00\"\n#&gt;  [40] \"2024-05-19T15:00\" \"2024-05-19T16:00\" \"2024-05-19T17:00\"\n#&gt;  [43] \"2024-05-19T18:00\" \"2024-05-19T19:00\" \"2024-05-19T20:00\"\n#&gt;  [46] \"2024-05-19T21:00\" \"2024-05-19T22:00\" \"2024-05-19T23:00\"\n#&gt;  [49] \"2024-05-20T00:00\" \"2024-05-20T01:00\" \"2024-05-20T02:00\"\n#&gt;  [52] \"2024-05-20T03:00\" \"2024-05-20T04:00\" \"2024-05-20T05:00\"\n#&gt;  [55] \"2024-05-20T06:00\" \"2024-05-20T07:00\" \"2024-05-20T08:00\"\n#&gt;  [58] \"2024-05-20T09:00\" \"2024-05-20T10:00\" \"2024-05-20T11:00\"\n#&gt;  [61] \"2024-05-20T12:00\" \"2024-05-20T13:00\" \"2024-05-20T14:00\"\n#&gt;  [64] \"2024-05-20T15:00\" \"2024-05-20T16:00\" \"2024-05-20T17:00\"\n#&gt;  [67] \"2024-05-20T18:00\" \"2024-05-20T19:00\" \"2024-05-20T20:00\"\n#&gt;  [70] \"2024-05-20T21:00\" \"2024-05-20T22:00\" \"2024-05-20T23:00\"\n#&gt;  [73] \"2024-05-21T00:00\" \"2024-05-21T01:00\" \"2024-05-21T02:00\"\n#&gt;  [76] \"2024-05-21T03:00\" \"2024-05-21T04:00\" \"2024-05-21T05:00\"\n#&gt;  [79] \"2024-05-21T06:00\" \"2024-05-21T07:00\" \"2024-05-21T08:00\"\n#&gt;  [82] \"2024-05-21T09:00\" \"2024-05-21T10:00\" \"2024-05-21T11:00\"\n#&gt;  [85] \"2024-05-21T12:00\" \"2024-05-21T13:00\" \"2024-05-21T14:00\"\n#&gt;  [88] \"2024-05-21T15:00\" \"2024-05-21T16:00\" \"2024-05-21T17:00\"\n#&gt;  [91] \"2024-05-21T18:00\" \"2024-05-21T19:00\" \"2024-05-21T20:00\"\n#&gt;  [94] \"2024-05-21T21:00\" \"2024-05-21T22:00\" \"2024-05-21T23:00\"\n#&gt;  [97] \"2024-05-22T00:00\" \"2024-05-22T01:00\" \"2024-05-22T02:00\"\n#&gt; [100] \"2024-05-22T03:00\" \"2024-05-22T04:00\" \"2024-05-22T05:00\"\n#&gt; [103] \"2024-05-22T06:00\" \"2024-05-22T07:00\" \"2024-05-22T08:00\"\n#&gt; [106] \"2024-05-22T09:00\" \"2024-05-22T10:00\" \"2024-05-22T11:00\"\n#&gt; [109] \"2024-05-22T12:00\" \"2024-05-22T13:00\" \"2024-05-22T14:00\"\n#&gt; [112] \"2024-05-22T15:00\" \"2024-05-22T16:00\" \"2024-05-22T17:00\"\n#&gt; [115] \"2024-05-22T18:00\" \"2024-05-22T19:00\" \"2024-05-22T20:00\"\n#&gt; [118] \"2024-05-22T21:00\" \"2024-05-22T22:00\" \"2024-05-22T23:00\"\n#&gt; [121] \"2024-05-23T00:00\" \"2024-05-23T01:00\" \"2024-05-23T02:00\"\n#&gt; [124] \"2024-05-23T03:00\" \"2024-05-23T04:00\" \"2024-05-23T05:00\"\n#&gt; [127] \"2024-05-23T06:00\" \"2024-05-23T07:00\" \"2024-05-23T08:00\"\n#&gt; [130] \"2024-05-23T09:00\" \"2024-05-23T10:00\" \"2024-05-23T11:00\"\n#&gt; [133] \"2024-05-23T12:00\" \"2024-05-23T13:00\" \"2024-05-23T14:00\"\n#&gt; [136] \"2024-05-23T15:00\" \"2024-05-23T16:00\" \"2024-05-23T17:00\"\n#&gt; [139] \"2024-05-23T18:00\" \"2024-05-23T19:00\" \"2024-05-23T20:00\"\n#&gt; [142] \"2024-05-23T21:00\" \"2024-05-23T22:00\" \"2024-05-23T23:00\"\n#&gt; [145] \"2024-05-24T00:00\" \"2024-05-24T01:00\" \"2024-05-24T02:00\"\n#&gt; [148] \"2024-05-24T03:00\" \"2024-05-24T04:00\" \"2024-05-24T05:00\"\n#&gt; [151] \"2024-05-24T06:00\" \"2024-05-24T07:00\" \"2024-05-24T08:00\"\n#&gt; [154] \"2024-05-24T09:00\" \"2024-05-24T10:00\" \"2024-05-24T11:00\"\n#&gt; [157] \"2024-05-24T12:00\" \"2024-05-24T13:00\" \"2024-05-24T14:00\"\n#&gt; [160] \"2024-05-24T15:00\" \"2024-05-24T16:00\" \"2024-05-24T17:00\"\n#&gt; [163] \"2024-05-24T18:00\" \"2024-05-24T19:00\" \"2024-05-24T20:00\"\n#&gt; [166] \"2024-05-24T21:00\" \"2024-05-24T22:00\" \"2024-05-24T23:00\"\n#&gt; \n#&gt; $hourly$temperature_2m\n#&gt;   [1] 16.4 15.5 14.4 13.6 12.8 12.1 11.9 12.8 14.7 17.0 19.2 20.7 20.7 20.8 21.3\n#&gt;  [16] 20.9 22.0 21.9 21.9 18.9 17.9 17.2 16.0 15.1 14.5 14.0 13.4 13.0 12.6 12.2\n#&gt;  [31] 12.0 12.4 13.3 14.5 16.4 18.6 19.8 20.2 20.5 20.7 20.9 21.0 19.3 19.6 18.8\n#&gt;  [46] 17.7 16.6 15.5 14.5 13.7 13.2 12.7 12.4 12.3 12.2 12.9 14.1 15.2 17.6 19.1\n#&gt;  [61] 18.5 18.9 19.9 20.6 20.5 20.1 19.8 18.2 17.6 16.8 15.9 15.4 15.1 14.8 14.6\n#&gt;  [76] 14.6 14.6 14.6 14.6 14.8 15.9 17.1 18.6 19.7 20.9 22.0 22.9 23.6 24.1 24.0\n#&gt;  [91] 23.1 21.7 20.4 19.3 18.3 17.4 16.7 16.1 15.7 15.2 14.8 14.8 15.4 16.4 17.5\n#&gt; [106] 19.0 20.6 21.9 22.9 23.6 23.8 23.1 21.8 20.7 20.2 19.8 19.4 18.8 18.3 17.8\n#&gt; [121] 17.5 17.2 16.8 16.0 15.0 14.5 14.8 15.6 16.2 16.1 16.2 16.3 16.5 16.6 16.6\n#&gt; [136] 16.5 16.3 16.1 15.8 15.4 15.2 15.1 15.1 15.0 14.8 14.5 14.2 13.9 13.6 13.4\n#&gt; [151] 13.2 13.2 13.4 14.0 15.0 16.0 17.0 18.1 19.1 19.8 20.3 20.5 20.3 19.9 19.3\n#&gt; [166] 18.5 17.6 16.6\n\n# Extract relevant data from the response\nforecast_data &lt;- respTable$hourly\n\n# Create a tibble\nweather_tbl &lt;- tibble(\n  time = forecast_data$time,\n  temperature_2m = forecast_data$temperature_2m\n)\n\n# Print the tibble\nprint(weather_tbl)\n\n#&gt; # A tibble: 168 × 2\n#&gt;    time             temperature_2m\n#&gt;    &lt;chr&gt;                     &lt;dbl&gt;\n#&gt;  1 2024-05-18T00:00           16.4\n#&gt;  2 2024-05-18T01:00           15.5\n#&gt;  3 2024-05-18T02:00           14.4\n#&gt;  4 2024-05-18T03:00           13.6\n#&gt;  5 2024-05-18T04:00           12.8\n#&gt;  6 2024-05-18T05:00           12.1\n#&gt;  7 2024-05-18T06:00           11.9\n#&gt;  8 2024-05-18T07:00           12.8\n#&gt;  9 2024-05-18T08:00           14.7\n#&gt; 10 2024-05-18T09:00           17  \n#&gt; # ℹ 158 more rows\n\n# Convert time to datetime format\nweather_tbl$time &lt;- as.POSIXct(weather_tbl$time, format = \"%Y-%m-%dT%H:%M\")\n\n# Ensure data is sorted by time\nweather_tbl &lt;- weather_tbl %&gt;% arrange(time)\n\n\n# Extract day part of the date\nweather_tbl$day &lt;- factor(format(weather_tbl$time, \"%d\"), levels = unique(format(weather_tbl$time, \"%d\")))\nweather_tbl &lt;- na.omit(weather_tbl)\n\n\n# Create a plot\np &lt;- ggplot(weather_tbl, aes(x = time, y = temperature_2m)) +\n  geom_point(position = \"identity\") +  # Set position to \"identity\" to remove space between points\n  geom_line(color = \"blue\", show.legend = FALSE) +\n  labs(title = \"Hourly Temperature for 7 Days\",\n       subtitle = \"Ordered by date and time\",\n       x = \"Time\",\n       y = \"Temperature (°C)\") +\n  theme_minimal() +\n  facet_wrap(~ factor(format(weather_tbl$time, \"%A\"), levels = unique(format(weather_tbl$time, \"%A\"))), nrow = 1, scales = \"free_x\")\n\n# Adjust x-axis labels to show only time with \":00\"\np &lt;- p + scale_x_datetime(date_breaks = \"6 hours\", date_labels = \"%H:%M:00\") # Adjust date_labels\n\n# Remove space between plots\np &lt;- p + theme(strip.text = element_text(size = 8, face = \"bold\"), \n               panel.spacing = unit(0, \"lines\"),\n               axis.text.x = element_text(angle = 45, hjust = 1, size = 8), # Adjust angle and size\n               legend.title = element_blank())\n\n# Print the plot\nprint(p)\n\n\n\n\n\n\n\n\n2 Challenge 2\nQuestion 2:\nScrape one of the competitor websites of canyon (either https://www.rosebikes.de/ or https://www.radon-bikes.de) and create a small database. The database should contain the model names and prices for at least one category. Use the selectorgadget to get a good understanding of the website structure, it is really helpful. After scraping your data, convert it to a readable format. Prices should be in a numeric format without any other letters or symbols. Also check if the prices are reasonable.\nAnswer:\nFor this I get the webpages of the racing, endurance, and e-bikes of the website rosebikes. They are found using the specific nodes for race, endurance, and e-bike categories. We further get titles and prices and strip them off trailing characters. Lastly, these will be printed.\nCode:\n\n# Challenge 2\n\n# Define the URL of the website\nurl_race &lt;- \"https://www.rosebikes.de/fahrr%C3%A4der/rennrad/race\"\nurl_endurance &lt;- \"https://www.rosebikes.de/fahrr%C3%A4der/rennrad/endurance\"\nurl_eBike &lt;- \"https://www.rosebikes.de/fahrr%C3%A4der/rennrad/e-rennrad\"\n\n# Read the HTML content of the webpage\nwebpage_race &lt;- url_race %&gt;% read_html()\nwebpage_endurance &lt;- url_endurance %&gt;% read_html()\nwebpage_eBike &lt;- url_eBike %&gt;% read_html()\n\n# Extract the content within the h4 tag with class \"basic-headline__title\"\ntitle_race        &lt;- html_text(html_nodes(webpage_race, \"h4.basic-headline__title\"))\ntitle_endurance   &lt;- html_text(html_nodes(webpage_endurance, \"h4.basic-headline__title\"))\ntitle_eBike       &lt;- html_text(html_nodes(webpage_eBike, \"h4.basic-headline__title\"))\n\n# Extract the content within the specified class \"catalog-category-bikes__price-title\"\nprice_title_race      &lt;- html_text(html_nodes(webpage_race, \".catalog-category-bikes__price-title\"))\nprice_title_endurance &lt;- html_text(html_nodes(webpage_endurance, \".catalog-category-bikes__price-title\"))\nprice_title_eBike&lt;- html_text(html_nodes(webpage_eBike, \".catalog-category-bikes__price-title\"))\n\n# Remove \"ab \" and \"\\n\" from the price\nprice_title_race &lt;- gsub(\"ab |\\\\n\", \"\", price_title_race)\nprice_title_endurance &lt;- gsub(\"ab |\\\\n\", \"\", price_title_endurance)\nprice_title_eBike&lt;- gsub(\"ab |\\\\n\", \"\", price_title_eBike)\n\n# Remove trailing whitespaces\nprice_title_race &lt;- trimws(price_title_race)\nprice_title_endurance &lt;- trimws(price_title_endurance)\nprice_title_eBike&lt;- trimws(price_title_eBike)\n\n\n# Create a tibble with the extracted content\nbike_data_race &lt;- tibble(model = title_race, price = price_title_race)\nbike_data_endurance &lt;- tibble(model = title_endurance, price = price_title_endurance)\nbike_data_eBike&lt;- tibble(model = title_eBike, price = price_title_eBike)\n\n\n# Print the tibble\nprint(bike_data_race)\n\n#&gt; # A tibble: 4 × 2\n#&gt;   model                      price     \n#&gt;   &lt;chr&gt;                      &lt;chr&gt;     \n#&gt; 1 XLITE UNLTD                8.999,00 €\n#&gt; 2 XLITE UNLTD KITTEL EDITION 8.999,00 €\n#&gt; 3 XLITE                      3.599,00 €\n#&gt; 4 XLITE 06 Rahmenset         2.999,00 €\n\nprint(bike_data_endurance)\n\n#&gt; # A tibble: 4 × 2\n#&gt;   model     price     \n#&gt;   &lt;chr&gt;     &lt;chr&gt;     \n#&gt; 1 BLEND     1.199,00 €\n#&gt; 2 REVEAL AL 1.849,00 €\n#&gt; 3 PRO SL    1.099,00 €\n#&gt; 4 REVEAL    3.299,00 €\n\nprint(bike_data_eBike)\n\n#&gt; # A tibble: 1 × 2\n#&gt;   model       price     \n#&gt;   &lt;chr&gt;       &lt;chr&gt;     \n#&gt; 1 REVEAL PLUS 3.699,00 €"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "1 Challenge 1\nQuestion 1:\nMap the time course of the cumulative Covid-19 cases!\nAnswer:\nFor this I read the COVID data and select the specific countries, which are Germany, UK, US, France and Spain. I additionally compute the cumulative COVID cases for Europe (optional). These will be plotted from 01.01.2020 to 20.04.2022 (20.04.2022 because at the 21.04.2022 the next month with missing data will be plotted too, so the limit is at the 20th of each month). What is plotted are the accumulations of COVID cases over the time frame for each country and Europe. A different theme than default is also used. For the two highest lines the last value will also be put out.\nCode:\n\n# Challenge 1\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(ggrepel) \n\ncovid_data_tbl &lt;- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#&gt; Rows: 399276 Columns: 67\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr   (4): iso_code, continent, location, tests_units\n#&gt; dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#&gt; date  (1): date\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data for the specified countries and date range\nselected_countries &lt;- c(\"Germany\", \"United Kingdom\", \"United States\", \"France\", \"Spain\")\n\nfiltered_data &lt;- covid_data_tbl %&gt;%\n  filter(location %in% selected_countries,\n         date &gt;= \"2020-01-01\" & date &lt;= \"2022-04-20\")\n\neurope_data &lt;- covid_data_tbl %&gt;%\n  filter(continent == \"Europe\",\n         date &gt;= \"2020-01-01\" & date &lt;= \"2022-04-20\") %&gt;%\n  group_by(date) %&gt;%\n  summarise(total_cases = ifelse(sum(total_cases, na.rm = TRUE) == 0, NA, sum(total_cases, na.rm = TRUE)))\n\n\n# Combine the Europe data with the original data\ncombined_data &lt;- bind_rows(filtered_data, europe_data %&gt;% mutate(location = \"Europe\"))\n\n# Define custom colors for the lines\ncustom_colors &lt;- c(\"red2\", \"skyblue2\", \"coral\", \"orange3\", \"yellow4\", 'deeppink4')  # You can add more colors as needed\n\n# Extract the last data points for Europe and United States\nlast_values &lt;- combined_data %&gt;%\n  filter(location %in% c(\"Europe\", \"United States\")) %&gt;%\n  group_by(location) %&gt;%\n  slice(n())  # Extract the last row for each location\n\n\n# Plot cumulative Covid-19 cases over time\nggplot(combined_data, aes(x = date, y = total_cases, color = location)) +\n  geom_line(size = 1.5, alpha = 0) +  # Increase size and reduce transparency\n  geom_smooth(method = \"loess\", se = FALSE, size = 1, alpha = 0.2, span = 0.2) +  # Adjust span for reduced smoothing\n  geom_label(data = last_values, aes(label = last_values$total_cases, color = location), \n             label.padding = unit(0.2, \"lines\"), \n             size = 3, \n             show.legend = FALSE,\n             label=format(last_values$total_cases, big.mark = \".\", scientific = FALSE)) + # Apply formatting to total_cases label\n  labs(title = \"Cumulative Covid-19 Cases Over Time\",\n       x = \"Date\",\n       y = \"Cumulative Cases\",\n       color = \"Country\") +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b %Y\") + # Set breaks and labels to every month\n  scale_y_continuous(labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), x),\n                     limits = c(0, 200e6)) +\n  theme_linedraw() +\n  scale_color_manual(values = custom_colors) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1), title = element_text(face = \"bold\", color = \"#0b1562\")) \n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n#&gt; Warning in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L, :\n#&gt; 'big.mark' und 'decimal.mark' sind beide '.', was verwirrend sein könnte\n\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n#&gt; Warning: Removed 84 rows containing non-finite outside the scale range\n#&gt; (`stat_smooth()`).\n\n\n#&gt; Warning: Removed 84 rows containing missing values or values outside the scale range\n#&gt; (`geom_line()`).\n\n\n#&gt; Warning: Removed 26 rows containing missing values or values outside the scale range\n#&gt; (`geom_smooth()`).\n\n\n\n\n\n\n\n\n\n2 Challenge 2\nQuestion 2:\nVisualize the distribution of the mortality rate (deaths / population) with geom_map() (alternatively, you can plot the case-fatality rate (deaths / cases)).\nAnswer:\nFor this I read the COVID data again and filter it to be the last proper day where data of entries is present as of time of this challenge. Then the country names are adjusted and the mortality rate will be computed. This is plotted using the world map with a red color scale. Here: light red -&gt; lower mortality rate, dark red -&gt; high mortality rate, low and high here are relative to the scale.\nCode:\n\n# Load map data\nworld &lt;- map_data(\"world\")\n\n# Read COVID-19 data\ncovid_data_tbl &lt;- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#&gt; Rows: 399276 Columns: 67\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr   (4): iso_code, continent, location, tests_units\n#&gt; dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#&gt; date  (1): date\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data for the specified date range\nfiltered_data &lt;- covid_data_tbl %&gt;%\n  filter(date == \"2024-04-28\")\n\n# Adjust country names\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(location = case_when(\n    location == \"United Kingdom\" ~ \"UK\",\n    location == \"United States\" ~ \"USA\",\n    location == \"Democratic Republic of Congo\" ~ \"Democratic Republic of the Congo\",\n    TRUE ~ location\n  )) %&gt;%\n  distinct()\n\n\n# Calculate mortality rate\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(mortality_rate = ifelse(population == 0, NA, total_deaths / population))\n\n# Merge COVID-19 data with map data\nmerged_data &lt;- left_join(world, filtered_data, by = c(\"region\" = \"location\"))\n\n\n# Plot distribution of mortality rate\nggplot() +\n  geom_map(data = merged_data, map = world, aes(map_id = region, fill = mortality_rate)) +\n  expand_limits(x = world$long, y = world$lat) +\n  scale_fill_gradient(name = \"Mortality Rate\", \n                      low = \"#ff1d1d\", \n                      high = \"#460000\", \n                      na.value = \"grey\", \n                      breaks = seq(0, 0.006, by = 0.002),  # Set breaks from 0 to 0.006 in steps of 0.001\n                      labels = paste0(seq(0.00, 0.60, by = 0.20), \"%\")) +  # Set labels from 0% to 60%\n  labs(title = \"Confirmed COVID-19 deaths relative to size of the population\",\n       fill = \"Mortality Rate\",\n       map_id = \"Country\") +\n  theme_void()"
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "Following links can be found on this webpage:\nHome\nJournal 01\nJournal 02\nJournal 03\nJournal 04\nClass notes\nLinks"
  },
  {
    "objectID": "content/03_other/06_links.html#r-and-r-studio",
    "href": "content/03_other/06_links.html#r-and-r-studio",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual ."
  },
  {
    "objectID": "content/03_other/06_links.html#additional-r-resources",
    "href": "content/03_other/06_links.html#additional-r-resources",
    "title": "Links",
    "section": "",
    "text": "Google is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html#header-2",
    "href": "content/01_journal/04_data_visualization.html#header-2",
    "title": "Data Visualization",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#header-2",
    "href": "content/01_journal/02_data_acquisition.html#header-2",
    "title": "Data Acquisition",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#header-2",
    "href": "content/01_journal/03_data_wrangling.html#header-2",
    "title": "Data Wrangling",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#question-1",
    "href": "content/01_journal/03_data_wrangling.html#question-1",
    "title": "Data Wrangling",
    "section": "",
    "text": "Patent Dominance: What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.\nAnswer:\nFor this we use the reduced data set due to computational restrictions. The data sets will be read in and wrangled and filtered according to US-company specific keywords. Then the companies with the most patents will be printed.\nCode:\n\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(vroom)\nlibrary(magrittr) \n\n\n# Challenge\ncol_types &lt;- list(\n  id = col_character(),\n  type = col_character(),\n  number = col_character(),\n  country = col_character(),\n  date = col_date(\"%Y-%m-%d\"),\n  abstract = col_character(),\n  title = col_character(),\n  kind = col_character(),\n  num_claims = col_double(),\n  filename = col_character(),\n  withdrawn = col_double()\n)\n\npatent_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/patent.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: type,\n#&gt; number, country, abstract, title, kind, filename, withdrawn\n\npatent_assignee_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/patent_assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: id, type,\n#&gt; number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\nassignee_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/assignee.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: number,\n#&gt; country, date, abstract, title, kind, num_claims, filename, withdrawn\n\nuspc_tbl &lt;- vroom(\n  file       = \"../../scripts/Patent_data_reduced/uspc.tsv\", \n  delim      = \"\\t\", \n  col_types  = col_types,\n  na         = c(\"\", \"NA\", \"NULL\")\n)\n\n#&gt; Warning: The following named parsers don't match the column names: id, type,\n#&gt; number, country, date, abstract, title, kind, num_claims, filename, withdrawn\n\n# Question 1\n# For 1 assignee, patent_assignee\n# Merge tables and count patents per organization\nmerged_tbl &lt;- merge(assignee_tbl, patent_assignee_tbl, by.x = \"id\", by.y = \"assignee_id\", all.x = TRUE)\n\n# Filter US companies and count patents\n# US companies are being filtered by US specific keywords\npatent_counts &lt;- merged_tbl %&gt;%\n  filter(grepl(\"Inc\\\\.|LLC|Corporation|Corporated|Company|Ltd\\\\.|Incorporated\", organization, ignore.case = TRUE)) %&gt;%\n  group_by(organization) %&gt;%\n  summarise(patent_count = n()) %&gt;%\n  arrange(desc(patent_count))\n\n# Print the result\nprint(patent_counts)\n\n#&gt; # A tibble: 28,689 × 2\n#&gt;    organization                                patent_count\n#&gt;    &lt;chr&gt;                                              &lt;int&gt;\n#&gt;  1 International Business Machines Corporation         7547\n#&gt;  2 Samsung Electronics Co., Ltd.                       5835\n#&gt;  3 Sony Corporation                                    3326\n#&gt;  4 Microsoft Corporation                               3165\n#&gt;  5 Google Inc.                                         2668\n#&gt;  6 QUALCOMM Incorporated                               2597\n#&gt;  7 LG Electronics Inc.                                 2459\n#&gt;  8 Panasonic Corporation                               2218\n#&gt;  9 Apple Inc.                                          2201\n#&gt; 10 General Electric Company                            1873\n#&gt; # ℹ 28,679 more rows"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#question-2",
    "href": "content/01_journal/03_data_wrangling.html#question-2",
    "title": "Data Wrangling",
    "section": "",
    "text": "Question:\nRecent patent activity: What US company had the most patents granted in August 2014? List the top 10 companies with the most new granted patents for August 2014.\nAnswer:\nFor this I filtered the patents for August 2014 and US only. Then I summarized the number of their patents to get the companies that had the most patents. These are printed out.\nCode:\n\n# For 2 assignee, patent_assignee, patent\n# Filter patents granted in August 2014\naugust_patents &lt;- patent_tbl %&gt;%\n  filter(format(date, \"%Y-%m\") == \"2014-08\")\n\n# Join with patent_assignee_tbl to get assignee information\npatents_assignees &lt;- august_patents %&gt;%\n  inner_join(patent_assignee_tbl, by = c(\"id\" = \"patent_id\"))\n\n# Join with assignee_tbl to get assignee information\npatents_assignees_details &lt;- patents_assignees %&gt;%\n  inner_join(assignee_tbl, by = c(\"assignee_id\" = \"id\"))\n\n# Filter only US companies\nus_companies &lt;- patents_assignees_details %&gt;%\n  filter(grepl(\"USA\", organization, ignore.case = TRUE))\n\n# Count patents granted to each US company\npatents_count &lt;- us_companies %&gt;%\n  group_by(organization) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents))\n\n# Select top 10 companies\ntop_10_us_companies &lt;- head(patents_count, 10)\n\nprint(top_10_us_companies)\n\n#&gt; # A tibble: 10 × 2\n#&gt;    organization                                                      num_patents\n#&gt;    &lt;chr&gt;                                                                   &lt;int&gt;\n#&gt;  1 Siemens Medical Solutions USA, Inc.                                        12\n#&gt;  2 ECOLAB USA INC.                                                             9\n#&gt;  3 HITACHI KOKUSAI ELECTRIC INC.                                               8\n#&gt;  4 United Services Automobile Association (USAA)                               6\n#&gt;  5 Ecole Polytechnique Federale de Lausanne (EPFL)                             3\n#&gt;  6 Philip Morris USA Inc.                                                      3\n#&gt;  7 Yissum Research Development Company of the Hebrew University of …           3\n#&gt;  8 CAUSAM ENERGY, INC.                                                         2\n#&gt;  9 Hendrickson USA, L.L.C.                                                     2\n#&gt; 10 Musashi Engineering, Inc.                                                   2"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#question-3",
    "href": "content/01_journal/03_data_wrangling.html#question-3",
    "title": "Data Wrangling",
    "section": "",
    "text": "Question:\nInnovation in Tech: What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?\nAnswer:\nFor this the the companies with the most number of patents will be taken and then for those the top company classes will be taken to be printed out, which are the top 5 tech classes of the top 10 companies.\nCode:\n\n# Question 3\n# For 3 assignee_tbl, patent_assignee_tbl, uspc_tbl\n# Find top 10 companies with most patents\ntop_10_companies &lt;- patent_assignee_tbl %&gt;%\n  group_by(assignee_id) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents)) %&gt;%\n  top_n(10)\n\n#&gt; Selecting by num_patents\n\n# Join with assignee_tbl to get org information\ntop_10_companies_with_org &lt;- top_10_companies %&gt;%\n  inner_join(select(assignee_tbl, -type), by = c(\"assignee_id\" = \"id\"))\n\n\n# Main classes of patents\ntop_10_main_classes &lt;- top_10_companies_with_org %&gt;%\n  inner_join(patent_assignee_tbl, by = \"assignee_id\") %&gt;%\n  inner_join(uspc_tbl %&gt;% mutate(patent_id = as.character(patent_id)), by = \"patent_id\") %&gt;%\n  group_by(mainclass_id) %&gt;%\n  summarise(num_patents = n()) %&gt;%\n  arrange(desc(num_patents)) %&gt;%\n  top_n(5)\n\n#&gt; Warning in inner_join(., uspc_tbl %&gt;% mutate(patent_id = as.character(patent_id)), : Detected an unexpected many-to-many relationship between `x` and `y`.\n#&gt; ℹ Row 1 of `x` matches multiple rows in `y`.\n#&gt; ℹ Row 373502 of `y` matches multiple rows in `x`.\n#&gt; ℹ If a many-to-many relationship is expected, set `relationship =\n#&gt;   \"many-to-many\"` to silence this warning.\n\n\n#&gt; Selecting by num_patents\n\nprint(top_10_main_classes)\n\n#&gt; # A tibble: 5 × 2\n#&gt;   mainclass_id num_patents\n#&gt;          &lt;dbl&gt;       &lt;int&gt;\n#&gt; 1          257        7956\n#&gt; 2          455        6120\n#&gt; 3          370        5448\n#&gt; 4          348        4102\n#&gt; 5          709        4010"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "My Lab Journal",
    "section": "Table of Contents",
    "text": "Table of Contents\nYou can find the challenges in the Journal\nThe scripts used for these challenges can be found in /scripts/"
  }
]